
# Test, Train, & Validation Dataset

## Understanding the basics of dataset splitting

Category: AI & ML | Date: 2024-03-06

![data-splitting](/images/240306/data-splitting.png)

&nbsp;

*This blog post is translated from my original blog post written in Korean, found [here](https://pozalabs.github.io/Dataset_Splitting/).*

&nbsp;

### What is Data?

Before learning about dataset splitting, let's briefly understand what data is. Wikipedia defines data as follows:

> ”data are a set of values of qualitative or quantitative variables about one or more persons or objects.”

When we describe the weather, we quantify it in terms of temperature, humidity, etc., and each of these numerical values is data. In the context of artificial intelligence, we feed such data or 'values' into our models to train them and achieve desired results. Therefore, in artificial intelligence, data can be considered the basic ingredient of models and the most crucial element.

&nbsp;


### Why do we need to split data?
When we develop models, our aim is to create ones that can be universally applied. The goal is to predict or classify unseen data. Here, unseen data refers to datasets that our model has not seen before, or data it has not been trained on. Since the model will be predicting or classifying new data when it is actually applied, we understand the importance of the model's performance on unseen data being good.

&nbsp;

Therefore, if we do not split the dataset and use it entirely for training, we will end up creating a model that performs well only on that specific dataset. Not only will we be unable to check the performance of the developed model, but we also cannot expect good performance when the model is applied in real situations. Hence, how we divide and use our dataset is very important.

&nbsp;

How should we split the dataset then? The most basic method of splitting is to divide the entire dataset into a training set and a test set. 80% of the entire dataset is used as the training set, and the remaining 20% as the test set. This is done to train the model using the training set and evaluate the model's performance using the test set.

&nbsp;

However, if we only split the dataset into training and test sets, we can only perform model performance validation once. Modifying the model based on the results from the test set increases the likelihood of overfitting. Therefore, we further divide the training set into a training set and a validation set. As shown in the figure below, in most cases, the entire dataset is divided into training, validation, and test sets using a 6:2:2 ratio, respectively.



#### Training Set:
- The training set is used solely for training the model. It is utilized to train the model and during the process of modifying parameters or features to enhance the model's performance.

&nbsp;

#### Validation Set:
- The validation set and the test set share the commonality of not being directly involved in training the model. However, the validation set is used for fine-tuning the model after training has been completed, before moving on to evaluating the model with the test set. Even for a trained model, its performance can vary depending on how many epochs are run or the settings of the learning rate. Therefore, we use the validation set to ensure that our model performs well on the test set and when it is actually deployed.


#### Test Set:
- The test set is the dataset used to evaluate the performance of our final model, and it is not used in training the model at all. Through the test set, we assess how well our model will perform in real-world usage after it has been deployed. As mentioned earlier, for a model to be successful in actual use, it is crucial that it performs well on unseen data. The test set acts as unseen data, helping us effectively evaluate the performance of our model.


&nbsp;

### Division of datasets according to their characteristics
So far, we have learned how to conduct basic dataset splitting. However, to maximize the learning effect and performance of the model, it is crucial to understand the amount and characteristics of the data we use for training and to divide it accordingly. From now on, we will explore how to efficiently split datasets based on the various characteristics that data can possess.
&nbsp;

#### Based on amount of Data
Firstly, we will examine how to split datasets based on the amount of data provided.

&nbsp;


##### Case of having a large amount of data
First, let's discuss the case when we have a large amount of data available. I want to mention that using a 6:2:2 ratio for dividing the dataset is somewhat an outdated method of dataset splitting. Compared to just a decade ago, thanks to the advancement of computer hardware and the ever-increasing accumulation of data, we are said to be living in the era of "big data," meaning the amount of data we can handle has significantly increased. Therefore, we are no longer restricted to using the 6:2:2 ratio and have become able to split datasets more efficiently.

&nbsp;


For example, if the size of the dataset we are dealing with is less than 100, 1000, or 10,000, splitting the dataset in a 6:2:2 ratio would allocate 20, 200, and 2,000 data points to each validation and test set, respectively. However, if the size of the dataset reaches 1,000,000, using 20% for each validation and test set would result in a staggering 200,000 data points for each. Even though the same 6:2:2 ratio is used, we can see a large variance in the size of each dataset depending on the total size of the dataset.

&nbsp;


Therefore, when splitting datasets, we need to consider the type of model we intend to create, or the tasks we want it to perform, and then decide how much data is needed for the validation and test sets accordingly. Let's assume that the amount of data needed to effectively evaluate the performance of our model is 2,000. If the total size of our dataset is 1,000,000, allocating just 0.2% to each the validation and test sets would be sufficient to obtain 2,000 data points for each. 

&nbsp;

In contrast, dividing the dataset into a 6:2:2 ratio, regardless of the total dataset size, would allocate as much as 200,000 data points to each the validation and test sets. This would reduce the amount of data available for training the model, which is not an efficient use of the dataset in developing a good model. Therefore, we should not be confined to the conventional 6:2:2 ratio and instead, consider the total dataset size along with the amount of data needed for each validation and test set to perform effective dataset splitting.

&nbsp;

##### Case of having a small amount of data

Now, let's discuss what to do in the case of having a small amount of data. If the size of the dataset given to us is excessively small, dividing it into a 6:2:2 ratio could result in each training, validation, and test set being allocated too few data points, making it difficult to effectively train and validate the model. In such cases, there are various dataset splitting techniques that can be used, but here, we will explore the k-fold technique.

&nbsp;

![k-fold](/images/240306/k-fold.png)

As shown in the figure above, if you fold the entire dataset k-1 times, it will be divided into k folds. The example shown is of folding the dataset 4 times to create a 5-fold dataset. (In most cases, 5-fold or 10-fold is used.)

&nbsp;


When training the model with the divided dataset, in each iteration, the test set is changed without duplication, as shown above, to repeat the model's training and testing. In the case of the example given, this process is repeated 5 times, and the k-fold technique involves averaging the validation results from each test fold to evaluate the model's performance.

&nbsp;


Using this method to split and utilize the dataset has the advantage of preventing overfitting and allowing the model to learn and be evaluated on different training and test data, especially when the amount of data is small. This approach enables learning across a diverse set of data. However, there is also a downside: the number of iterations increases, leading to longer training times.


#### Based on characteristics of data
Machine learning and deep learning are being utilized in a wide range of fields recently, leading us to handle various types and different properties of data. Therefore, the method of dataset splitting can vary slightly depending on what kind of data we are using to develop the model.

&nbsp;

![table-1](/images/240306/table-1.png)

Taking time-series data as an example among the various data characteristics mentioned above, let's assume we are developing a model to predict stock prices. Stock prices change over time, and the data we use will be a dataset where stock prices are stored for each point in time. Data whose order is meaningful and important is called time-series data. In dealing with such data where order matters, if we were to split the dataset and randomly shuffle the data, it would become impossible for our model to learn anything meaningful. Therefore, when training a model with time-series data, we must not randomly mix the data but rather allow the model to learn from the data as it changes over time.

&nbsp;


While time-series data was used as an example here, it's important to note that the required method for dataset splitting can vary depending on the type and characteristics of the data, not just for time-series data. Therefore, when splitting datasets, it's necessary to consider the properties of the data we are dealing with.

#### Based on the distribution of the data and the nature of the target
The method of splitting a dataset can vary slightly based on the distribution of the data and the nature of the target. Before discussing this further, let's understand what is meant by distribution and target here:
  
  &nbsp;
  
  - Distribution refers to the source of the data (such as web pages, user-uploaded images, data collected in the United States, data collected in Korea, etc.).
  - Target denotes the problem we aim to solve through the model or the goal that enables the model to perform well.

- When we develop and research models, there is a target we aim to achieve through the model. To create a successful model after its deployment, it is crucial to allocate the most similar data to the real-world application data to the validation and test sets. The similar data mentioned here refers to the distribution of the data that the model will encounter when it is actually used. For example, let's assume we are developing a model for an application intended for use in South Korea. If we train the model with data collected not only from South Korea but also from the United States, South Africa, France, etc. (i.e., from different distributions), the performance of our model in South Korea might not be as good when used there compared to if we had developed the model using data collected solely from South Korea.

If there is enough data, such concerns are unnecessary. When sourcing data suitable for our target, we can simply ensure that the train, validation, and test sets all come from the same distribution. However, there may be instances where it is impossible to maintain the same distribution due to a lack of data suitable for our target. In such cases, we have two options:

&nbsp;

  * The first option is to randomly mix datasets A and B and then divide them into training, validation, and test sets.
    * Advantages: Since the entire data comes from the same distribution, it allows for a uniform division of the training, validation, and test sets in terms of their properties.
    * Disadvantages: Since it's not data coming from the distribution where the model will actually be used, the result may deviate from the desired target, potentially leading to lower accuracy.
  * The second option is to use dataset A for the validation and test sets, and dataset B for the training set.
    * Advantages: The results obtained from the validation set can be considered similar to the performance when the model is actually used.
    * Disadvantages: Because the properties of the training set and the validation set that the model was trained on can be different, it may take longer to improve the model's performance.



- Although both options exist, each with its own advantages and disadvantages, we recommend choosing the second option. Even if the distributions for the training set and the validation/test sets are different, it is more important for the developed model to perform successfully when actually used. Therefore, we believe it is better to allocate datasets to the validation/test sets that more accurately reflect the data the model will encounter in real-world usage.

&nbsp;
### Evaluation Metric
Finally, let's briefly look into the evaluation metrics used to assess the results on the validation set after training the model.

&nbsp;

Evaluation metrics are used to measure the quality of the statistical or machine learning model. When a model trained on the training set is validated through the validation set, we assess its performance using evaluation metrics. An evaluation metric, as defined earlier, is a criterion that helps us understand the efficiency and accuracy of our model's performance. Through evaluation metrics, we can determine whether the model will perform well when applied in real situations. Therefore, it is considered crucial to properly establish and choose the right evaluation metrics.

&nbsp;


Indeed, the appropriate evaluation metric can vary depending on the problem we aim to solve with the model. The commonly used or representative evaluation metrics vary across different fields of machine learning and depending on the task. Below, through a table, we will briefly introduce the evaluation metrics commonly or representatively used in various machine learning tasks.

&nbsp;

![table-2](/images/240306/table-2.png)

Assuming we have well-established evaluation metrics that accurately reflect how successful our model will be in real-world applications, if the results based on these metrics are not satisfactory, it's also a good approach to re-evaluate whether the dataset was split effectively. If it's determined that the dataset splitting was not done efficiently or appropriately, reconsidering how the dataset is divided and using it again could also help improve the model's performance.

### Closing Notes

Through this post, we've explored what dataset splitting is and its importance. We discussed the roles and differences between training, validation, and test sets, talked about traditional ways of splitting datasets versus current trends, and explained various factors that should be considered when splitting datasets. Beyond what has been explained, how we handle and use data when developing deep learning models is a critical aspect. Therefore, it would be beneficial to study other methods of division and the characteristics of each dataset in the future.






































































































